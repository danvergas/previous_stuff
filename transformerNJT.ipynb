{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.attention.flex_attention import flex_attention, create_nested_block_mask\n",
        "\n",
        "def generate_alibi_bias(H: int):\n",
        "    \"\"\"Returns an alibi bias score_mod given the number of heads H\n",
        "    Args:\n",
        "        H: number of heads\n",
        "    Returns:\n",
        "        alibi_bias: alibi bias score_mod\n",
        "    \"\"\"\n",
        "\n",
        "    def alibi_mod(score, b, h, q_idx, kv_idx):\n",
        "        scale = torch.exp2(-((h + 1) * 8.0 / H))\n",
        "        bias = (q_idx - kv_idx) * scale\n",
        "        return score + bias\n",
        "\n",
        "    return alibi_mod\n",
        "\n",
        "\n",
        "def causal_mask(b, h, q_idx, kv_idx):\n",
        "    return q_idx >= kv_idx\n",
        "\n",
        "\n",
        "class MultiHeadAttentionALIBI(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes multi-head attention with ALIBI. Supports nested tensors.\n",
        "\n",
        "    Args:\n",
        "        dim: Size of embedding dim for query, key and value\n",
        "        num_head (int): Number of heads\n",
        "        dropout (float, optional): Dropout probability. Default: 0.0\n",
        "        bias (bool, optional): Whether to add bias to input projection. Default: True\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_head: int,\n",
        "        dropout: float = 0.0,\n",
        "        bias=True,\n",
        "        device='cpu'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "        self.dropout = dropout\n",
        "        assert dim % num_head == 0, \"Dim is not divisible by number of heads\"\n",
        "        self.dim_head = dim // num_head\n",
        "        self.packed_proj = nn.Linear(dim, dim * 3, bias=bias, device=device)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=bias, device=device)\n",
        "        self.bias = bias\n",
        "        self.score_mode = generate_alibi_bias(num_head)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        is_causal=False,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass; runs the following process:\n",
        "            1. Apply input projection\n",
        "            2. Split heads and prepare for flex attn\n",
        "            3. Run flex attn\n",
        "            4. Apply output projection\n",
        "        Args:\n",
        "            N = batch size\n",
        "            query (torch.Tensor): query of shape (``N``, ``L*``, ``dim``)\n",
        "            key (torch.Tensor): key of shape (``N``, ``L*``, ``dim``)\n",
        "            value (torch.Tensor): value of shape (``N``, ``L*``, ``dim``)\n",
        "            is_causal (bool, optional): Whether to apply causal mask. Default: False\n",
        "        Returns:\n",
        "            attn_output (torch.Tensor): output of shape (N, L*, dim)\n",
        "        \"\"\"\n",
        "        N = query.size(0)\n",
        "        # Step 1. Apply input projection\n",
        "        if query is key and key is value:\n",
        "            result = self.packed_proj(query)\n",
        "            query, key, value = torch.chunk(result, 3, dim=-1)\n",
        "        else:\n",
        "            q_weight, k_weight, v_weight = torch.chunk(\n",
        "                self.packed_proj.weight, 3, dim=0\n",
        "            )\n",
        "            if self.bias:\n",
        "                q_bias, k_bias, v_bias = torch.chunk(\n",
        "                    self.packed_proj.bias, 3, dim=0\n",
        "                )\n",
        "            else:\n",
        "                q_bias, k_bias, v_bias = None, None, None\n",
        "            query, key, value = (\n",
        "                F.linear(query, q_weight, q_bias),\n",
        "                F.linear(key, k_weight, k_bias),\n",
        "                F.linear(value, v_weight, v_bias)\n",
        "            )\n",
        "\n",
        "        # Step 2. Split heads and prepare for flex attn\n",
        "        # reshape query, key, value to separate by head\n",
        "        # (N, L*, dim) -> (N, L*, num_head, dim_head) -> (N, num_head, L*, dim_head)\n",
        "        query = query.unflatten(-1, [self.num_head, self.dim_head]).transpose(1, 2).detach().requires_grad_()\n",
        "        key = key.unflatten(-1, [self.num_head, self.dim_head]).transpose(1, 2).detach().requires_grad_()\n",
        "        value = value.unflatten(-1, [self.num_head, self.dim_head]).transpose(1, 2).detach().requires_grad_()\n",
        "\n",
        "\n",
        "        block_mask = None\n",
        "        if (is_causal):\n",
        "          block_mask = create_nested_block_mask(causal_mask, N, self.num_head, query, key, _compile=True)\n",
        "        # Step 3. Run flex attn\n",
        "        # (N, num_head, L*, dim_head)\n",
        "        attn_output = flex_attention(query, key, value, score_mod=self.score_mode, block_mask=block_mask)\n",
        "        # (N, num_head, L*, dim_head) -> (N, L*, num_head, dim_head) -> (N, L*, dim)\n",
        "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
        "\n",
        "        # Step 4. Apply output projection\n",
        "        # (N, L*, dim) -> (N, L*, dim)\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "5v3tI-t-7OaN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iouk1yKph7AS"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        hidden_dim, dropout=0.1, device='cpu'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.fc1 = nn.Linear(self.dim, self.hidden_dim, bias = True, device=device)\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "        self.fc2 = nn.Linear(self.hidden_dim, self.dim, bias = True, device=device)\n",
        "        self.dropout2 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # for potential speed up\n",
        "        # Pre-normalize the weights (can help with training stability)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.dropout2(self.fc2(self.dropout1(F.relu(self.fc1(input)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YIetnJg3dw44"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, hidden_dim,\n",
        "        num_head, dropout=0.1,\n",
        "        bias=True, device='cpu'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "\n",
        "        # attention sublayer\n",
        "        self.self_attention = MultiHeadAttentionALIBI(\n",
        "            dim = dim,\n",
        "            num_head = num_head,\n",
        "            dropout = dropout,\n",
        "            bias = bias,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # FFN sublayer\n",
        "        self.ffn = FFN(\n",
        "            dim = dim,\n",
        "            hidden_dim = hidden_dim,\n",
        "            dropout = dropout,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # layer-normalization layer\n",
        "        self.LayerNorm_att = nn.LayerNorm(self.dim, device=device)\n",
        "        self.LayerNorm_ffn = nn.LayerNorm(self.dim, device=device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        att = self.self_attention(x, x, x)\n",
        "        att = self.dropout(att)\n",
        "        att_normalized = self.LayerNorm_att(x + att)\n",
        "\n",
        "        ffn_sublayer = self.ffn(att_normalized)\n",
        "        ffn_normalized = self.LayerNorm_ffn(att_normalized + ffn_sublayer)\n",
        "\n",
        "        return ffn_normalized\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_head, num_layers, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(dim, hidden_dim, num_head, dropout, device=device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, src):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d9NcRI0wcWUx"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, dim, hidden_dim,\n",
        "        num_head, dropout=0.1,\n",
        "        bias=True, device='cpu'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "\n",
        "        self.self_attention = MultiHeadAttentionALIBI(\n",
        "            dim = dim,\n",
        "            num_head = num_head,\n",
        "            dropout = dropout,\n",
        "            bias = bias,\n",
        "            device=device\n",
        "        )\n",
        "        self.cross_attention = MultiHeadAttentionALIBI(\n",
        "            dim = dim,\n",
        "            num_head = num_head,\n",
        "            dropout = dropout,\n",
        "            bias = bias,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # FFN sublayer\n",
        "        self.ffn = FFN(\n",
        "            dim = dim,\n",
        "            hidden_dim = hidden_dim,\n",
        "            dropout=dropout,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout_self_att = nn.Dropout(p=dropout)\n",
        "        self.dropout_cross_att = nn.Dropout(p=dropout)\n",
        "\n",
        "        # layer-normalization layer\n",
        "        self.LayerNorm_self_att = nn.LayerNorm(self.dim, device=device)\n",
        "        self.LayerNorm_cross_att = nn.LayerNorm(self.dim, device=device)\n",
        "        self.LayerNorm_ffn = nn.LayerNorm(self.dim, device=device)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "\n",
        "        #calculate self attnention\n",
        "        sa = self.self_attention(tgt, tgt, tgt, is_causal=True)\n",
        "        sa = self.dropout_self_att(sa)\n",
        "        sa_norm = self.LayerNorm_self_att(tgt + sa)\n",
        "\n",
        "        #calculate cross attnention\n",
        "        ca = self.cross_attention(sa_norm, memory, memory)\n",
        "        ca = self.dropout_cross_att(ca)\n",
        "        ca_norm = self.LayerNorm_cross_att(ca+sa_norm)\n",
        "\n",
        "        #calculate ffn\n",
        "        res = self.ffn(ca_norm)\n",
        "        res_norm = self.LayerNorm_ffn(res+ca_norm)\n",
        "        return res_norm\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_head, num_layers, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(dim, hidden_dim, num_head, dropout, device=device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory)\n",
        "        return tgt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IZb2PaKbhGO7"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_head, num_layers, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.enc = Encoder(dim, hidden_dim, num_head, num_layers, dropout, device=device)\n",
        "        self.dec = Decoder(dim, hidden_dim, num_head, num_layers, dropout, device=device)\n",
        "    def forward(self, src, tgt):\n",
        "        memory = self.enc(src)\n",
        "        output = self.dec(tgt, memory)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aXfzFPfWhGO8"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_head, num_layers, src_voc, tgt_vocab, src_emb, tgt_emb, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.src_embed=nn.Embedding(src_voc, src_emb, device=device)\n",
        "        self.tgt_embed=nn.Embedding(tgt_vocab, tgt_emb, device=device)\n",
        "        self.src_proj = nn.Linear(src_emb, dim, device=device)\n",
        "        self.tgt_proj = nn.Linear(tgt_emb, dim, device=device)\n",
        "        self.enc = Encoder(dim, hidden_dim, num_head, num_layers, dropout, device=device)\n",
        "        self.dec = Decoder(dim, hidden_dim, num_head, num_layers, dropout, device=device)\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_proj(src)\n",
        "        memory = self.enc(src)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_proj(tgt)\n",
        "        output = self.dec(tgt, memory)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9gMByXOWhGO9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def zipf_sentence_lengths(alpha: float, batch_size: int) -> torch.Tensor:\n",
        "    # generate fake corpus by unigram Zipf distribution\n",
        "    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n",
        "    sentence_lengths = np.empty(batch_size, dtype=int)\n",
        "    for ibatch in range(batch_size):\n",
        "        sentence_lengths[ibatch] = 1\n",
        "        word = np.random.zipf(alpha)\n",
        "        while word != 3 and word != 386 and word != 858:\n",
        "            sentence_lengths[ibatch] += 1\n",
        "            word = np.random.zipf(alpha)\n",
        "    return torch.tensor(sentence_lengths)\n",
        "\n",
        "\n",
        "# Generate a batch of semi-realistic data using Zipf distribution for sentence lengths\n",
        "# in the form of nested tensors with the jagged layout.\n",
        "def gen_batch(N, vocab_size, device, dtype=torch.long):\n",
        "    # generate semi-realistic data using Zipf distribution for sentence lengths\n",
        "    sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N)\n",
        "\n",
        "    # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged\n",
        "    # dimension and works with torch.compile. The batch items each have shape (B, S*, D)\n",
        "    # where B = batch size, S* = ragged sequence length, and D = embedding dimension.\n",
        "    batch = torch.nested.nested_tensor(\n",
        "        [torch.randint(0, vocab_size, (l,), dtype=dtype, device=device) for l in sentence_lengths],\n",
        "        layout=torch.jagged, device=device\n",
        "    )\n",
        "\n",
        "    return batch, sentence_lengths\n",
        "\n",
        "import math\n",
        "import timeit\n",
        "\n",
        "\n",
        "def benchmark(func, *args, **kwargs):\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    begin = timeit.default_timer()\n",
        "    output = func(*args, **kwargs)\n",
        "    torch.cuda.synchronize()\n",
        "    end = timeit.default_timer()\n",
        "    return output, (end - begin), torch.cuda.max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "f7M_B1HMghLH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1VQTUzBhGO_",
        "outputId": "1a87325f-fd27-4837-c718-8da48862c156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequence length in nested src 1579, max sequence length 107\n",
            "Total sequence length in nested tg 1174, max sequence length 102\n",
            "padded_time=0.18868, padded_peak_memory=7.01 GB\n",
            "nested_time=1.31168, nested_peak_memory=4.05 GB\n",
            "Nested peak memory reduction 2.96 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class TransformerPad(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, num_head, num_layers, src_voc, tgt_vocab, src_emb, tgt_emb, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.src_embed=nn.Embedding(src_voc, src_emb, device=device)\n",
        "        self.tgt_embed=nn.Embedding(tgt_vocab, tgt_emb, device=device)\n",
        "        self.src_proj = nn.Linear(src_emb, dim, device=device)\n",
        "        self.tgt_proj = nn.Linear(tgt_emb, dim, device=device)\n",
        "        self.trans = nn.Transformer(d_model=dim, nhead=num_head, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True, device=device)\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_proj(src)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_proj(tgt)\n",
        "        output = self.trans(src, tgt)\n",
        "        return output\n",
        "\n",
        "b_size = 64\n",
        "src_vocab = 1754\n",
        "src_embed = 1024\n",
        "tgt_vocab = 1632\n",
        "tgt_embed = 1024\n",
        "dim = 512\n",
        "hidden_dim = 2048\n",
        "num_head = 8\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "bias = True\n",
        "device = \"cuda\"\n",
        "torch.device(device)\n",
        "torch.manual_seed(6)\n",
        "src, src_sentence_lengths  = gen_batch(b_size, src_vocab, device)\n",
        "tgt, tgt_sentence_lengths = gen_batch(b_size, tgt_vocab, device)\n",
        "Ssrc = src_sentence_lengths.max().item()\n",
        "Stgt = tgt_sentence_lengths.max().item()\n",
        "\n",
        "print(\n",
        "    f\"Total sequence length in nested src {src_sentence_lengths.sum().item()}, max sequence length {Ssrc}\"\n",
        ")\n",
        "print(\n",
        "    f\"Total sequence length in nested tg {tgt_sentence_lengths.sum().item()}, max sequence length {Stgt}\"\n",
        ")\n",
        "padded_src, padded_tgt = (\n",
        "    t.to_padded_tensor(0) for t in (src, tgt)\n",
        ")\n",
        "\n",
        "torch.manual_seed(6)\n",
        "trans_njt = Transformer(\n",
        "    dim, hidden_dim, num_head, num_layers, src_vocab, tgt_vocab, src_embed, tgt_embed, dropout=dropout, device=device\n",
        ")\n",
        "torch.manual_seed(6)\n",
        "trans = TransformerPad(\n",
        "    dim, hidden_dim, num_head, num_layers, src_vocab, tgt_vocab, src_embed, tgt_embed, dropout=dropout, device=device\n",
        ")\n",
        "\n",
        "# benchmark\n",
        "nested_result, nested_time, nested_peak_memory = benchmark(\n",
        "    trans_njt, src, tgt\n",
        ")\n",
        "padded_nested_result = nested_result.to_padded_tensor(0.0)\n",
        "\n",
        "# benchmark\n",
        "padded_result, padded_time, padded_peak_memory = benchmark(\n",
        "    trans,\n",
        "    padded_src,\n",
        "    padded_tgt\n",
        ")\n",
        "\n",
        "print(f\"{padded_time=:.5f}, padded_peak_memory={padded_peak_memory/1e9:.2f} GB\")\n",
        "print(f\"{nested_time=:.5f}, nested_peak_memory={nested_peak_memory/1e9:.2f} GB\")\n",
        "print(\n",
        "    f\"Nested peak memory reduction {((padded_peak_memory - nested_peak_memory)/1e9):.2f} GB\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yq_hwmk7I9_"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}